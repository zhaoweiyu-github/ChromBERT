{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example for causal eQTL identification using prompt-enhanced ChromBERT\n",
    "ChromBERT’s integration of a DNA sequence prompt from DNABERT-2 enhances its versatility for genomic applications, such as fine-mapping eQTLs.   \n",
    "By incorporating DNA sequence variation, ChromBERT can identify causal variants influencing gene expression.   \n",
    "Using the latest eQTL Catalogue, we fine-tuned ChromBERT to classify causal and non-causal variants as an example of its capability.\n",
    "\n",
    "**Attention: You should go through this [tutorial](https://chrombert.readthedocs.io/en/latest/tutorial_finetuning_ChromBERT.html) at first to get familiar with the basic usage of ChromBERT.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangdongxu/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import  os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" # to selected gpu used \n",
    "\n",
    "import sys \n",
    "import pathlib \n",
    "import pickle\n",
    "\n",
    "import torch \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import chrombert\n",
    "from torchinfo import summary\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "import sklearn \n",
    "from sklearn import metrics\n",
    "\n",
    "basedir = os.path.expanduser(\"~/.cache/chrombert/data\"  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets\n",
    "\n",
    "We provide a demo eQTL dataset for the lung, which includes additional columns compared to those used in other tasks: `base_ref`, `base_alt`, `variant_id`, `label`, and `pos`.\n",
    "\n",
    "- `pos`: Specifies the variant position.  \n",
    "- `base_ref`: Indicates the reference allele.  \n",
    "- `base_alt`: Represents the alternative allele.  \n",
    "\n",
    "The `variant_id` serves as a unique identifier for each variant and can be any unique string for convenience. The `label` column classifies variants as causal (1) or non-causal (0). If the dataset is used only for prediction, the `label` column can be omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrom\tstart\tend\tbuild_region_index\tbase_ref\tbase_alt\tvariant_id\tlabel\tpos\n",
      "chr17\t29595000\t29596000\t771566\tA\tG\tchr17_29595257_A_G\t1\t29595257\n",
      "chr11\t78139000\t78140000\t342669\tC\tT\tchr11_78139778_C_T\t1\t78139778\n",
      "chr19\t35309000\t35310000\t898784\tG\tT\tchr19_35309759_G_T\t1\t35309759\n",
      "chr4\t6695000\t6696000\t1359650\tT\tC\tchr4_6695946_T_C\t1\t6695946\n",
      "chr6\t139029000\t139030000\t1719458\tG\tA\tchr6_139029045_G_A\t1\t139029045\n",
      "chr15\t41554000\t41555000\t640548\tT\tG\tchr15_41554978_T_G\t1\t41554978\n",
      "chr16\t4343000\t4344000\t694493\tA\tC\tchr16_4343375_A_C\t1\t4343375\n",
      "chr9\t97238000\t97239000\t2025247\tA\tG\tchr9_97238449_A_G\t1\t97238449\n",
      "chr5\t96757000\t96758000\t1551457\tG\tA\tchr5_96757518_G_A\t1\t96757518\n"
     ]
    }
   ],
   "source": [
    "table_eqtls = os.path.join(basedir, \"demo\",\"eqtl\", \"lung_eqtl.tsv\")\n",
    "!head $table_eqtls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(921, 614)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "odir = pathlib.Path(\"tmp_eqtl\")\n",
    "odir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "df_full = pd.read_csv(table_eqtls, sep=\"\\t\")\n",
    "df_train, df_test = train_test_split(df_full, test_size=0.4, random_state=42)\n",
    "df_train.to_csv(odir / \"train.tsv\", sep=\"\\t\", index=False)\n",
    "df_test.to_csv(odir / \"test.tsv\", sep=\"\\t\", index=False)\n",
    "len(df_train), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update path: hdf5_file = hg38_6k_1kb.hdf5\n",
      "update path: meta_file = config/hg38_6k_meta.json\n",
      "update path: fasta_file = other/hg38.fa\n",
      "{\n",
      "    \"hdf5_file\": \"/home/yangdongxu/.cache/chrombert/data/hg38_6k_1kb.hdf5\",\n",
      "    \"supervised_file\": \"/home/yangdongxu/.cache/chrombert/data/demo/eqtl/lung_eqtl.tsv\",\n",
      "    \"kind\": \"PromptDataset\",\n",
      "    \"meta_file\": \"/home/yangdongxu/.cache/chrombert/data/config/hg38_6k_meta.json\",\n",
      "    \"ignore\": false,\n",
      "    \"ignore_object\": null,\n",
      "    \"batch_size\": 8,\n",
      "    \"num_workers\": 20,\n",
      "    \"shuffle\": false,\n",
      "    \"perturbation\": false,\n",
      "    \"perturbation_object\": null,\n",
      "    \"perturbation_value\": 0,\n",
      "    \"prompt_kind\": \"dna\",\n",
      "    \"prompt_regulator\": null,\n",
      "    \"prompt_regulator_cache_file\": null,\n",
      "    \"prompt_celltype\": null,\n",
      "    \"prompt_celltype_cache_file\": null,\n",
      "    \"fasta_file\": \"/home/yangdongxu/.cache/chrombert/data/other/hg38.fa\",\n",
      "    \"flank_window\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# configure dataset\n",
    "dc = chrombert.get_preset_dataset_config(\"prompt_dna\", supervised_file = table_eqtls)\n",
    "print(dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([9, 9, 9,  ..., 6, 6, 6], dtype=torch.int8),\n",
       " 'position_ids': tensor([   1,    2,    3,  ..., 6389, 6390, 6391]),\n",
       " 'region': tensor([      11, 78139000, 78140000], dtype=torch.int32),\n",
       " 'build_region_index': 342669,\n",
       " 'label': 1,\n",
       " 'seq_raw': 'GATTTGTTCCAAATCAGACAGCGCCAGGTCTGAACCTAGCCAGCTGGGGCTAAGTCAAGTAACAACTGGCGAAACAGAAAGCTTAGCAAAGGCAGGATAGCGACAAACACGACCTAAAGTTTTCTCTTCATACCCAGGGATATCCACACCTTTCTCTCCCGCCCTGACCGACCGCGGGGCCTCCCCGCCCAGCCCCTGGCCGTGCGAGTCCCTTACTATGTGGGGATGAGAAGGCATTTGAGAAGAGTCACCCCGAGCGCCAAAGCCGAAAACCAATTGCCAGTACCCGTGGCAATTGTGAGCGCCGCCATTGCTGCGGCACCGCACGCTTCCCACCAACTTGATCCACATCCGGGATCCCGCGCATGCGGAGAAAGCCCTCTGAAGCCGTGCCCGCTAGCTGCGCGCATGCGGCGAGCGGCGCAGCCAGTCCGGGGACTGCAGTCAGCTATTTAAACCTCCCGCCCACCTTTTCTTTAGACCCGCGTCTCACCCCGGGCCGGAAGGGCTCCTGCGCAGGCGTTTGTAGCCACTTTTAAGTTTTATCAGCTAGTTCATGCTTGCGTTGAAAGAGTGGTCGTTTGCGCTGGGTCATCACTGTGTAGTATTGGGGATACTTAGGTGAGAAAAAAACTTAACGCTAGAGACGTTCACGCACTAGTGGAGAAGCCAGGATTGTTGCCCTAGAGTTACAGTAGATAAAAGTACCTCAGAGAACTGCGGGGGCTCCCAACCTGGACGCTTGCACCGGAGTATTAAATCCAGCTAGAGAATGGCATGTGCAAAGATACAGAGGTGAGAAACATTGTGTTTTTAGAACTCTGAGCGAGGCTCTTGGCTCACCTCCTGCTTGAGCGGAACCCATTCTGGAAGCAGGGTAGAGGCTAGTCCTAACGCTTAGTGTACAAATAGCCTACGGTTCATGTTAAAATAATTCGGATTCTGATTCAGTAGGCCCAACAAACTCGCAGATTGCGTAATGACTGAGGCACATGCAATT',\n",
       " 'seq_alt': 'GATTTGTTCCAAATCAGACAGCGCCAGGTCTGAACCTAGCCAGCTGGGGCTAAGTCAAGTAACAACTGGCGAAACAGAAAGCTTAGCAAAGGCAGGATAGCGACAAACACGACCTAAAGTTTTCTCTTCATACCCAGGGATATCCACACCTTTCTCTCCCGCCCTGACCGACCGCGGGGCCTCCCCGCCCAGCCCCTGGCCGTGCGAGTCCCTTACTATGTGGGGATGAGAAGGCATTTGAGAAGAGTCACCCCGAGCGCCAAAGCCGAAAACCAATTGCCAGTACCCGTGGCAATTGTGAGCGCCGCCATTGCTGCGGCACCGCACGCTTCCCACCAACTTGATCCACATCCGGGATCCCGCGCATGCGGAGAAAGCCCTCTGAAGCCGTGCCCGCTAGCTGCGCGCATGCGGCGAGCGGCGCAGCCAGTCCGGGGACTGCAGTCAGCTATTTAAACCTCCCGCCCACCTTTTCTTTAGACCCGCGTCTCACCCCGGGCTGGAAGGGCTCCTGCGCAGGCGTTTGTAGCCACTTTTAAGTTTTATCAGCTAGTTCATGCTTGCGTTGAAAGAGTGGTCGTTTGCGCTGGGTCATCACTGTGTAGTATTGGGGATACTTAGGTGAGAAAAAAACTTAACGCTAGAGACGTTCACGCACTAGTGGAGAAGCCAGGATTGTTGCCCTAGAGTTACAGTAGATAAAAGTACCTCAGAGAACTGCGGGGGCTCCCAACCTGGACGCTTGCACCGGAGTATTAAATCCAGCTAGAGAATGGCATGTGCAAAGATACAGAGGTGAGAAACATTGTGTTTTTAGAACTCTGAGCGAGGCTCTTGGCTCACCTCCTGCTTGAGCGGAACCCATTCTGGAAGCAGGGTAGAGGCTAGTCCTAACGCTTAGTGTACAAATAGCCTACGGTTCATGTTAAAATAATTCGGATTCTGATTCAGTAGGCCCAACAAACTCGCAGATTGCGTAATGACTGAGGCACATGCAATT'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize dataset\n",
    "ds = dc.init_dataset()\n",
    "ds[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model\n",
    "\n",
    "The model can be loaded in the same way as for other tasks, except that the kind is set to `prompt_dna`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update path: mtx_mask = config/hg38_6k_mask_matrix.tsv\n",
      "update path: pretrain_ckpt = checkpoint/hg38_6k_1kb_pretrain.ckpt\n",
      "{\n",
      "    \"genome\": \"hg38\",\n",
      "    \"task\": \"prompt\",\n",
      "    \"dim_output\": 1,\n",
      "    \"mtx_mask\": \"/home/yangdongxu/.cache/chrombert/data/config/hg38_6k_mask_matrix.tsv\",\n",
      "    \"dropout\": 0.1,\n",
      "    \"pretrain_ckpt\": \"/home/yangdongxu/.cache/chrombert/data/checkpoint/hg38_6k_1kb_pretrain.ckpt\",\n",
      "    \"finetune_ckpt\": null,\n",
      "    \"ignore\": false,\n",
      "    \"ignore_index\": [\n",
      "        null,\n",
      "        null\n",
      "    ],\n",
      "    \"gep_flank_window\": 4,\n",
      "    \"gep_parallel_embedding\": false,\n",
      "    \"gep_gradient_checkpoint\": false,\n",
      "    \"gep_zero_inflation\": true,\n",
      "    \"prompt_kind\": \"dna\",\n",
      "    \"prompt_dim_external\": 768,\n",
      "    \"dnabert2_ckpt\": \"zhihan1996/DNABERT-2-117M\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "mc = chrombert.get_preset_model_config(\n",
    "    \"prompt_dna\",\n",
    "    dnabert2_ckpt=\"zhihan1996/DNABERT-2-117M\" # use model from hugging-face, or provide path directly\n",
    "    )\n",
    "print(mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use organisim hg38; max sequence length including cls is 6392\n",
      "Warning: zhihan1996/DNABERT-2-117M does not exist! Try to use huggingface cached...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangdongxu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Some weights of the model checkpoint at zhihan1996/DNABERT-2-117M were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "ChromBERTPromptDNA                                                --\n",
       "├─ChromBERT: 1-1                                                  --\n",
       "│    └─BERTEmbedding: 2-1                                         --\n",
       "│    │    └─TokenEmbedding: 3-1                                   7,680\n",
       "│    │    └─PositionalEmbedding: 3-2                              4,909,056\n",
       "│    │    └─Dropout: 3-3                                          --\n",
       "│    └─ModuleList: 2-2                                            --\n",
       "│    │    └─EncoderTransformerBlock: 3-4                          6,497,280\n",
       "│    │    └─EncoderTransformerBlock: 3-5                          6,497,280\n",
       "│    │    └─EncoderTransformerBlock: 3-6                          6,497,280\n",
       "│    │    └─EncoderTransformerBlock: 3-7                          6,497,280\n",
       "│    │    └─EncoderTransformerBlock: 3-8                          6,497,280\n",
       "│    │    └─EncoderTransformerBlock: 3-9                          6,497,280\n",
       "│    │    └─EncoderTransformerBlock: 3-10                         6,497,280\n",
       "│    │    └─EncoderTransformerBlock: 3-11                         6,497,280\n",
       "├─DNABERT2Interface: 1-2                                          --\n",
       "│    └─BertModel: 2-3                                             --\n",
       "│    │    └─BertEmbeddings: 3-12                                  3,148,800\n",
       "│    │    └─BertEncoder: 3-13                                     113,329,152\n",
       "│    │    └─BertPooler: 3-14                                      590,592\n",
       "├─AdapterExternalEmb: 1-3                                         --\n",
       "│    └─ResidualBlock: 2-4                                         --\n",
       "│    │    └─Linear: 3-15                                          590,592\n",
       "│    │    └─Linear: 3-16                                          590,592\n",
       "│    │    └─LayerNorm: 3-17                                       1,536\n",
       "│    │    └─Sequential: 3-18                                      --\n",
       "│    │    └─Dropout: 3-19                                         --\n",
       "│    └─ResidualBlock: 2-5                                         --\n",
       "│    │    └─Linear: 3-20                                          590,592\n",
       "│    │    └─Linear: 3-21                                          590,592\n",
       "│    │    └─LayerNorm: 3-22                                       1,536\n",
       "│    │    └─Sequential: 3-23                                      --\n",
       "│    │    └─Dropout: 3-24                                         --\n",
       "├─GeneralHeader: 1-4                                              --\n",
       "│    └─CistromeEmbeddingManager: 2-6                              --\n",
       "│    └─Conv2d: 2-7                                                769\n",
       "│    └─ReLU: 2-8                                                  --\n",
       "│    └─ResidualBlock: 2-9                                         --\n",
       "│    │    └─Linear: 3-25                                          1,090,560\n",
       "│    │    └─Linear: 3-26                                          1,049,600\n",
       "│    │    └─LayerNorm: 3-27                                       2,048\n",
       "│    │    └─Linear: 3-28                                          1,090,560\n",
       "│    │    └─Dropout: 3-29                                         --\n",
       "│    └─ResidualBlock: 2-10                                        --\n",
       "│    │    └─Linear: 3-30                                          787,200\n",
       "│    │    └─Linear: 3-31                                          590,592\n",
       "│    │    └─LayerNorm: 3-32                                       1,536\n",
       "│    │    └─Linear: 3-33                                          787,200\n",
       "│    │    └─Dropout: 3-34                                         --\n",
       "│    └─ResidualBlock: 2-11                                        --\n",
       "│    │    └─Linear: 3-35                                          196,864\n",
       "│    │    └─Linear: 3-36                                          65,792\n",
       "│    │    └─LayerNorm: 3-37                                       512\n",
       "│    │    └─Linear: 3-38                                          196,864\n",
       "│    │    └─Dropout: 3-39                                         --\n",
       "│    └─Linear: 2-12                                               257\n",
       "├─PromptHeader: 1-5                                               --\n",
       "│    └─Sequential: 2-13                                           --\n",
       "│    │    └─ResidualBlock: 3-40                                   4,724,736\n",
       "│    │    └─ResidualBlock: 3-41                                   2,952,960\n",
       "│    │    └─ResidualBlock: 3-42                                   1,182,720\n",
       "│    │    └─ResidualBlock: 3-43                                   102,720\n",
       "│    │    └─Linear: 3-44                                          65\n",
       "==========================================================================================\n",
       "Total params: 191,152,515\n",
       "Trainable params: 191,152,515\n",
       "Non-trainable params: 0\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mc.init_model()\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_params': 191152515, 'trainable_params': 74083971}\n",
      "pretrain_model.embedding.token.weight : trainable\n",
      "pretrain_model.embedding.position.pe.pe.weight : trainable\n",
      "pretrain_model.transformer_blocks.0.attention.Wqkv.weight : trainable\n",
      "pretrain_model.transformer_blocks.0.attention.Wqkv.bias : trainable\n",
      "pretrain_model.transformer_blocks.0.feed_forward.w_1.weight : trainable\n",
      "pretrain_model.transformer_blocks.0.feed_forward.w_1.bias : trainable\n",
      "pretrain_model.transformer_blocks.0.feed_forward.w_2.weight : trainable\n",
      "pretrain_model.transformer_blocks.0.feed_forward.w_2.bias : trainable\n",
      "pretrain_model.transformer_blocks.0.input_sublayer.norm.a_2 : trainable\n",
      "pretrain_model.transformer_blocks.0.input_sublayer.norm.b_2 : trainable\n",
      "pretrain_model.transformer_blocks.0.output_sublayer.norm.a_2 : trainable\n",
      "pretrain_model.transformer_blocks.0.output_sublayer.norm.b_2 : trainable\n",
      "pretrain_model.transformer_blocks.1.attention.Wqkv.weight : trainable\n",
      "pretrain_model.transformer_blocks.1.attention.Wqkv.bias : trainable\n",
      "pretrain_model.transformer_blocks.1.feed_forward.w_1.weight : trainable\n",
      "pretrain_model.transformer_blocks.1.feed_forward.w_1.bias : trainable\n",
      "pretrain_model.transformer_blocks.1.feed_forward.w_2.weight : trainable\n",
      "pretrain_model.transformer_blocks.1.feed_forward.w_2.bias : trainable\n",
      "pretrain_model.transformer_blocks.1.input_sublayer.norm.a_2 : trainable\n",
      "pretrain_model.transformer_blocks.1.input_sublayer.norm.b_2 : trainable\n",
      "pretrain_model.transformer_blocks.1.output_sublayer.norm.a_2 : trainable\n",
      "pretrain_model.transformer_blocks.1.output_sublayer.norm.b_2 : trainable\n",
      "pretrain_model.transformer_blocks.2.attention.Wqkv.weight : trainable\n",
      "pretrain_model.transformer_blocks.2.attention.Wqkv.bias : trainable\n",
      "pretrain_model.transformer_blocks.2.feed_forward.w_1.weight : trainable\n",
      "pretrain_model.transformer_blocks.2.feed_forward.w_1.bias : trainable\n",
      "pretrain_model.transformer_blocks.2.feed_forward.w_2.weight : trainable\n",
      "pretrain_model.transformer_blocks.2.feed_forward.w_2.bias : trainable\n",
      "pretrain_model.transformer_blocks.2.input_sublayer.norm.a_2 : trainable\n",
      "pretrain_model.transformer_blocks.2.input_sublayer.norm.b_2 : trainable\n",
      "pretrain_model.transformer_blocks.2.output_sublayer.norm.a_2 : trainable\n",
      "pretrain_model.transformer_blocks.2.output_sublayer.norm.b_2 : trainable\n",
      "pretrain_model.transformer_blocks.3.attention.Wqkv.weight : trainable\n",
      "pretrain_model.transformer_blocks.3.attention.Wqkv.bias : trainable\n",
      "pretrain_model.transformer_blocks.3.feed_forward.w_1.weight : trainable\n",
      "pretrain_model.transformer_blocks.3.feed_forward.w_1.bias : trainable\n",
      "pretrain_model.transformer_blocks.3.feed_forward.w_2.weight : trainable\n",
      "pretrain_model.transformer_blocks.3.feed_forward.w_2.bias : trainable\n",
      "pretrain_model.transformer_blocks.3.input_sublayer.norm.a_2 : trainable\n",
      "pretrain_model.transformer_blocks.3.input_sublayer.norm.b_2 : trainable\n",
      "pretrain_model.transformer_blocks.3.output_sublayer.norm.a_2 : trainable\n",
      "pretrain_model.transformer_blocks.3.output_sublayer.norm.b_2 : trainable\n",
      "pretrain_model.transformer_blocks.4.attention.Wqkv.weight : trainable\n",
      "pretrain_model.transformer_blocks.4.attention.Wqkv.bias : trainable\n",
      "pretrain_model.transformer_blocks.4.feed_forward.w_1.weight : trainable\n",
      "pretrain_model.transformer_blocks.4.feed_forward.w_1.bias : trainable\n",
      "pretrain_model.transformer_blocks.4.feed_forward.w_2.weight : trainable\n",
      "pretrain_model.transformer_blocks.4.feed_forward.w_2.bias : trainable\n",
      "pretrain_model.transformer_blocks.4.input_sublayer.norm.a_2 : trainable\n",
      "pretrain_model.transformer_blocks.4.input_sublayer.norm.b_2 : trainable\n",
      "pretrain_model.transformer_blocks.4.output_sublayer.norm.a_2 : trainable\n",
      "pretrain_model.transformer_blocks.4.output_sublayer.norm.b_2 : trainable\n",
      "pretrain_model.transformer_blocks.5.attention.Wqkv.weight : trainable\n",
      "pretrain_model.transformer_blocks.5.attention.Wqkv.bias : trainable\n",
      "pretrain_model.transformer_blocks.5.feed_forward.w_1.weight : trainable\n",
      "pretrain_model.transformer_blocks.5.feed_forward.w_1.bias : trainable\n",
      "pretrain_model.transformer_blocks.5.feed_forward.w_2.weight : trainable\n",
      "pretrain_model.transformer_blocks.5.feed_forward.w_2.bias : trainable\n",
      "pretrain_model.transformer_blocks.5.input_sublayer.norm.a_2 : trainable\n",
      "pretrain_model.transformer_blocks.5.input_sublayer.norm.b_2 : trainable\n",
      "pretrain_model.transformer_blocks.5.output_sublayer.norm.a_2 : trainable\n",
      "pretrain_model.transformer_blocks.5.output_sublayer.norm.b_2 : trainable\n",
      "pretrain_model.transformer_blocks.6.attention.Wqkv.weight : trainable\n",
      "pretrain_model.transformer_blocks.6.attention.Wqkv.bias : trainable\n",
      "pretrain_model.transformer_blocks.6.feed_forward.w_1.weight : trainable\n",
      "pretrain_model.transformer_blocks.6.feed_forward.w_1.bias : trainable\n",
      "pretrain_model.transformer_blocks.6.feed_forward.w_2.weight : trainable\n",
      "pretrain_model.transformer_blocks.6.feed_forward.w_2.bias : trainable\n",
      "pretrain_model.transformer_blocks.6.input_sublayer.norm.a_2 : trainable\n",
      "pretrain_model.transformer_blocks.6.input_sublayer.norm.b_2 : trainable\n",
      "pretrain_model.transformer_blocks.6.output_sublayer.norm.a_2 : trainable\n",
      "pretrain_model.transformer_blocks.6.output_sublayer.norm.b_2 : trainable\n",
      "pretrain_model.transformer_blocks.7.attention.Wqkv.weight : trainable\n",
      "pretrain_model.transformer_blocks.7.attention.Wqkv.bias : trainable\n",
      "pretrain_model.transformer_blocks.7.feed_forward.w_1.weight : trainable\n",
      "pretrain_model.transformer_blocks.7.feed_forward.w_1.bias : trainable\n",
      "pretrain_model.transformer_blocks.7.feed_forward.w_2.weight : trainable\n",
      "pretrain_model.transformer_blocks.7.feed_forward.w_2.bias : trainable\n",
      "pretrain_model.transformer_blocks.7.input_sublayer.norm.a_2 : trainable\n",
      "pretrain_model.transformer_blocks.7.input_sublayer.norm.b_2 : trainable\n",
      "pretrain_model.transformer_blocks.7.output_sublayer.norm.a_2 : trainable\n",
      "pretrain_model.transformer_blocks.7.output_sublayer.norm.b_2 : trainable\n",
      "dnabert2.model.embeddings.word_embeddings.weight : frozen\n",
      "dnabert2.model.embeddings.token_type_embeddings.weight : frozen\n",
      "dnabert2.model.embeddings.LayerNorm.weight : frozen\n",
      "dnabert2.model.embeddings.LayerNorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.0.attention.self.Wqkv.weight : frozen\n",
      "dnabert2.model.encoder.layer.0.attention.self.Wqkv.bias : frozen\n",
      "dnabert2.model.encoder.layer.0.attention.output.dense.weight : frozen\n",
      "dnabert2.model.encoder.layer.0.attention.output.dense.bias : frozen\n",
      "dnabert2.model.encoder.layer.0.attention.output.LayerNorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.0.attention.output.LayerNorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.0.mlp.gated_layers.weight : frozen\n",
      "dnabert2.model.encoder.layer.0.mlp.wo.weight : frozen\n",
      "dnabert2.model.encoder.layer.0.mlp.wo.bias : frozen\n",
      "dnabert2.model.encoder.layer.0.mlp.layernorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.0.mlp.layernorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.1.attention.self.Wqkv.weight : frozen\n",
      "dnabert2.model.encoder.layer.1.attention.self.Wqkv.bias : frozen\n",
      "dnabert2.model.encoder.layer.1.attention.output.dense.weight : frozen\n",
      "dnabert2.model.encoder.layer.1.attention.output.dense.bias : frozen\n",
      "dnabert2.model.encoder.layer.1.attention.output.LayerNorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.1.attention.output.LayerNorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.1.mlp.gated_layers.weight : frozen\n",
      "dnabert2.model.encoder.layer.1.mlp.wo.weight : frozen\n",
      "dnabert2.model.encoder.layer.1.mlp.wo.bias : frozen\n",
      "dnabert2.model.encoder.layer.1.mlp.layernorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.1.mlp.layernorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.2.attention.self.Wqkv.weight : frozen\n",
      "dnabert2.model.encoder.layer.2.attention.self.Wqkv.bias : frozen\n",
      "dnabert2.model.encoder.layer.2.attention.output.dense.weight : frozen\n",
      "dnabert2.model.encoder.layer.2.attention.output.dense.bias : frozen\n",
      "dnabert2.model.encoder.layer.2.attention.output.LayerNorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.2.attention.output.LayerNorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.2.mlp.gated_layers.weight : frozen\n",
      "dnabert2.model.encoder.layer.2.mlp.wo.weight : frozen\n",
      "dnabert2.model.encoder.layer.2.mlp.wo.bias : frozen\n",
      "dnabert2.model.encoder.layer.2.mlp.layernorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.2.mlp.layernorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.3.attention.self.Wqkv.weight : frozen\n",
      "dnabert2.model.encoder.layer.3.attention.self.Wqkv.bias : frozen\n",
      "dnabert2.model.encoder.layer.3.attention.output.dense.weight : frozen\n",
      "dnabert2.model.encoder.layer.3.attention.output.dense.bias : frozen\n",
      "dnabert2.model.encoder.layer.3.attention.output.LayerNorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.3.attention.output.LayerNorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.3.mlp.gated_layers.weight : frozen\n",
      "dnabert2.model.encoder.layer.3.mlp.wo.weight : frozen\n",
      "dnabert2.model.encoder.layer.3.mlp.wo.bias : frozen\n",
      "dnabert2.model.encoder.layer.3.mlp.layernorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.3.mlp.layernorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.4.attention.self.Wqkv.weight : frozen\n",
      "dnabert2.model.encoder.layer.4.attention.self.Wqkv.bias : frozen\n",
      "dnabert2.model.encoder.layer.4.attention.output.dense.weight : frozen\n",
      "dnabert2.model.encoder.layer.4.attention.output.dense.bias : frozen\n",
      "dnabert2.model.encoder.layer.4.attention.output.LayerNorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.4.attention.output.LayerNorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.4.mlp.gated_layers.weight : frozen\n",
      "dnabert2.model.encoder.layer.4.mlp.wo.weight : frozen\n",
      "dnabert2.model.encoder.layer.4.mlp.wo.bias : frozen\n",
      "dnabert2.model.encoder.layer.4.mlp.layernorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.4.mlp.layernorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.5.attention.self.Wqkv.weight : frozen\n",
      "dnabert2.model.encoder.layer.5.attention.self.Wqkv.bias : frozen\n",
      "dnabert2.model.encoder.layer.5.attention.output.dense.weight : frozen\n",
      "dnabert2.model.encoder.layer.5.attention.output.dense.bias : frozen\n",
      "dnabert2.model.encoder.layer.5.attention.output.LayerNorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.5.attention.output.LayerNorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.5.mlp.gated_layers.weight : frozen\n",
      "dnabert2.model.encoder.layer.5.mlp.wo.weight : frozen\n",
      "dnabert2.model.encoder.layer.5.mlp.wo.bias : frozen\n",
      "dnabert2.model.encoder.layer.5.mlp.layernorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.5.mlp.layernorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.6.attention.self.Wqkv.weight : frozen\n",
      "dnabert2.model.encoder.layer.6.attention.self.Wqkv.bias : frozen\n",
      "dnabert2.model.encoder.layer.6.attention.output.dense.weight : frozen\n",
      "dnabert2.model.encoder.layer.6.attention.output.dense.bias : frozen\n",
      "dnabert2.model.encoder.layer.6.attention.output.LayerNorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.6.attention.output.LayerNorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.6.mlp.gated_layers.weight : frozen\n",
      "dnabert2.model.encoder.layer.6.mlp.wo.weight : frozen\n",
      "dnabert2.model.encoder.layer.6.mlp.wo.bias : frozen\n",
      "dnabert2.model.encoder.layer.6.mlp.layernorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.6.mlp.layernorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.7.attention.self.Wqkv.weight : frozen\n",
      "dnabert2.model.encoder.layer.7.attention.self.Wqkv.bias : frozen\n",
      "dnabert2.model.encoder.layer.7.attention.output.dense.weight : frozen\n",
      "dnabert2.model.encoder.layer.7.attention.output.dense.bias : frozen\n",
      "dnabert2.model.encoder.layer.7.attention.output.LayerNorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.7.attention.output.LayerNorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.7.mlp.gated_layers.weight : frozen\n",
      "dnabert2.model.encoder.layer.7.mlp.wo.weight : frozen\n",
      "dnabert2.model.encoder.layer.7.mlp.wo.bias : frozen\n",
      "dnabert2.model.encoder.layer.7.mlp.layernorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.7.mlp.layernorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.8.attention.self.Wqkv.weight : frozen\n",
      "dnabert2.model.encoder.layer.8.attention.self.Wqkv.bias : frozen\n",
      "dnabert2.model.encoder.layer.8.attention.output.dense.weight : frozen\n",
      "dnabert2.model.encoder.layer.8.attention.output.dense.bias : frozen\n",
      "dnabert2.model.encoder.layer.8.attention.output.LayerNorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.8.attention.output.LayerNorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.8.mlp.gated_layers.weight : frozen\n",
      "dnabert2.model.encoder.layer.8.mlp.wo.weight : frozen\n",
      "dnabert2.model.encoder.layer.8.mlp.wo.bias : frozen\n",
      "dnabert2.model.encoder.layer.8.mlp.layernorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.8.mlp.layernorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.9.attention.self.Wqkv.weight : frozen\n",
      "dnabert2.model.encoder.layer.9.attention.self.Wqkv.bias : frozen\n",
      "dnabert2.model.encoder.layer.9.attention.output.dense.weight : frozen\n",
      "dnabert2.model.encoder.layer.9.attention.output.dense.bias : frozen\n",
      "dnabert2.model.encoder.layer.9.attention.output.LayerNorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.9.attention.output.LayerNorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.9.mlp.gated_layers.weight : frozen\n",
      "dnabert2.model.encoder.layer.9.mlp.wo.weight : frozen\n",
      "dnabert2.model.encoder.layer.9.mlp.wo.bias : frozen\n",
      "dnabert2.model.encoder.layer.9.mlp.layernorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.9.mlp.layernorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.10.attention.self.Wqkv.weight : frozen\n",
      "dnabert2.model.encoder.layer.10.attention.self.Wqkv.bias : frozen\n",
      "dnabert2.model.encoder.layer.10.attention.output.dense.weight : frozen\n",
      "dnabert2.model.encoder.layer.10.attention.output.dense.bias : frozen\n",
      "dnabert2.model.encoder.layer.10.attention.output.LayerNorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.10.attention.output.LayerNorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.10.mlp.gated_layers.weight : frozen\n",
      "dnabert2.model.encoder.layer.10.mlp.wo.weight : frozen\n",
      "dnabert2.model.encoder.layer.10.mlp.wo.bias : frozen\n",
      "dnabert2.model.encoder.layer.10.mlp.layernorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.10.mlp.layernorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.11.attention.self.Wqkv.weight : frozen\n",
      "dnabert2.model.encoder.layer.11.attention.self.Wqkv.bias : frozen\n",
      "dnabert2.model.encoder.layer.11.attention.output.dense.weight : frozen\n",
      "dnabert2.model.encoder.layer.11.attention.output.dense.bias : frozen\n",
      "dnabert2.model.encoder.layer.11.attention.output.LayerNorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.11.attention.output.LayerNorm.bias : frozen\n",
      "dnabert2.model.encoder.layer.11.mlp.gated_layers.weight : frozen\n",
      "dnabert2.model.encoder.layer.11.mlp.wo.weight : frozen\n",
      "dnabert2.model.encoder.layer.11.mlp.wo.bias : frozen\n",
      "dnabert2.model.encoder.layer.11.mlp.layernorm.weight : frozen\n",
      "dnabert2.model.encoder.layer.11.mlp.layernorm.bias : frozen\n",
      "dnabert2.model.pooler.dense.weight : frozen\n",
      "dnabert2.model.pooler.dense.bias : frozen\n",
      "adapter_dna_emb.fc1.fc1.weight : trainable\n",
      "adapter_dna_emb.fc1.fc1.bias : trainable\n",
      "adapter_dna_emb.fc1.fc2.weight : trainable\n",
      "adapter_dna_emb.fc1.fc2.bias : trainable\n",
      "adapter_dna_emb.fc1.norm.a_2 : trainable\n",
      "adapter_dna_emb.fc1.norm.b_2 : trainable\n",
      "adapter_dna_emb.fc2.fc1.weight : trainable\n",
      "adapter_dna_emb.fc2.fc1.bias : trainable\n",
      "adapter_dna_emb.fc2.fc2.weight : trainable\n",
      "adapter_dna_emb.fc2.fc2.bias : trainable\n",
      "adapter_dna_emb.fc2.norm.a_2 : trainable\n",
      "adapter_dna_emb.fc2.norm.b_2 : trainable\n",
      "adapter_chrombert.conv.weight : trainable\n",
      "adapter_chrombert.conv.bias : trainable\n",
      "adapter_chrombert.res1.fc1.weight : trainable\n",
      "adapter_chrombert.res1.fc1.bias : trainable\n",
      "adapter_chrombert.res1.fc2.weight : trainable\n",
      "adapter_chrombert.res1.fc2.bias : trainable\n",
      "adapter_chrombert.res1.norm.a_2 : trainable\n",
      "adapter_chrombert.res1.norm.b_2 : trainable\n",
      "adapter_chrombert.res1.shortcut.weight : trainable\n",
      "adapter_chrombert.res1.shortcut.bias : trainable\n",
      "adapter_chrombert.res2.fc1.weight : trainable\n",
      "adapter_chrombert.res2.fc1.bias : trainable\n",
      "adapter_chrombert.res2.fc2.weight : trainable\n",
      "adapter_chrombert.res2.fc2.bias : trainable\n",
      "adapter_chrombert.res2.norm.a_2 : trainable\n",
      "adapter_chrombert.res2.norm.b_2 : trainable\n",
      "adapter_chrombert.res2.shortcut.weight : trainable\n",
      "adapter_chrombert.res2.shortcut.bias : trainable\n",
      "adapter_chrombert.res3.fc1.weight : trainable\n",
      "adapter_chrombert.res3.fc1.bias : trainable\n",
      "adapter_chrombert.res3.fc2.weight : trainable\n",
      "adapter_chrombert.res3.fc2.bias : trainable\n",
      "adapter_chrombert.res3.norm.a_2 : trainable\n",
      "adapter_chrombert.res3.norm.b_2 : trainable\n",
      "adapter_chrombert.res3.shortcut.weight : trainable\n",
      "adapter_chrombert.res3.shortcut.bias : trainable\n",
      "adapter_chrombert.fc.weight : trainable\n",
      "adapter_chrombert.fc.bias : trainable\n",
      "head_output.fcs.0.fc1.weight : trainable\n",
      "head_output.fcs.0.fc1.bias : trainable\n",
      "head_output.fcs.0.fc2.weight : trainable\n",
      "head_output.fcs.0.fc2.bias : trainable\n",
      "head_output.fcs.0.norm.a_2 : trainable\n",
      "head_output.fcs.0.norm.b_2 : trainable\n",
      "head_output.fcs.1.fc1.weight : trainable\n",
      "head_output.fcs.1.fc1.bias : trainable\n",
      "head_output.fcs.1.fc2.weight : trainable\n",
      "head_output.fcs.1.fc2.bias : trainable\n",
      "head_output.fcs.1.norm.a_2 : trainable\n",
      "head_output.fcs.1.norm.b_2 : trainable\n",
      "head_output.fcs.1.shortcut.weight : trainable\n",
      "head_output.fcs.1.shortcut.bias : trainable\n",
      "head_output.fcs.2.fc1.weight : trainable\n",
      "head_output.fcs.2.fc1.bias : trainable\n",
      "head_output.fcs.2.fc2.weight : trainable\n",
      "head_output.fcs.2.fc2.bias : trainable\n",
      "head_output.fcs.2.norm.a_2 : trainable\n",
      "head_output.fcs.2.norm.b_2 : trainable\n",
      "head_output.fcs.3.fc1.weight : trainable\n",
      "head_output.fcs.3.fc1.bias : trainable\n",
      "head_output.fcs.3.fc2.weight : trainable\n",
      "head_output.fcs.3.fc2.bias : trainable\n",
      "head_output.fcs.3.norm.a_2 : trainable\n",
      "head_output.fcs.3.norm.b_2 : trainable\n",
      "head_output.fcs.3.shortcut.weight : trainable\n",
      "head_output.fcs.3.shortcut.bias : trainable\n",
      "head_output.fcs.4.weight : trainable\n",
      "head_output.fcs.4.bias : trainable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 191152515, 'trainable_params': 74083971}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we freeze DNABERT-2 model during traing \n",
    "model.dnabert2.freeze()\n",
    "model.display_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune\n",
    "\n",
    "This task has fewer training samples compared to others, so we use a reduced number of training steps. To simplify the process, the model is trained directly without using PyTorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update path: hdf5_file = hg38_6k_1kb.hdf5\n",
      "update path: meta_file = config/hg38_6k_meta.json\n",
      "update path: fasta_file = other/hg38.fa\n",
      "update path: hdf5_file = hg38_6k_1kb.hdf5\n",
      "update path: meta_file = config/hg38_6k_meta.json\n",
      "update path: fasta_file = other/hg38.fa\n"
     ]
    }
   ],
   "source": [
    "dc_train = chrombert.get_preset_dataset_config(\"prompt_dna\", supervised_file = df_train)\n",
    "dc_test = chrombert.get_preset_dataset_config(\"prompt_dna\", supervised_file = df_test)\n",
    "\n",
    "ds_train = dc_train.init_dataset()\n",
    "ds_test = dc_test.init_dataset()\n",
    "dl_train = dc_train.init_dataloader(batch_size=2, shuffle=True, num_workers=4)\n",
    "dl_test = dc_test.init_dataloader(batch_size=2, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup \n",
    "from torch import nn\n",
    " \n",
    "def train(m, dl, lr = 5e-5, grad_accumulation_steps=4, min_epochs = 2, max_epochs = 5, max_steps = 200):\n",
    "    num_training_steps = len(dl) * max_epochs // grad_accumulation_steps\n",
    "    optimizer = torch.optim.AdamW(m.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "    loss_f = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    keys_to_cuda = [\"input_ids\", \"position_ids\", \"label\"]\n",
    "    total_steps = 0\n",
    "    for e in range(max_epochs):\n",
    "        m.train()\n",
    "        for i, batch in enumerate(tqdm(dl)):\n",
    "            # batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            for k in keys_to_cuda:\n",
    "                batch[k] = batch[k].cuda()\n",
    "\n",
    "            logits = m(batch).view(-1)\n",
    "            loss = loss_f(logits, batch[\"label\"].to(torch.float))\n",
    "            loss = loss.mean() / grad_accumulation_steps\n",
    "            loss.backward()\n",
    "            if (i+1) % grad_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                total_steps += 1\n",
    "                if total_steps > max_steps:\n",
    "                    if e >= min_epochs:\n",
    "                        return m\n",
    "\n",
    "        print(f\"epoch {e} loss {loss.item()}\")\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/461 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 461/461 [01:18<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 0.032958984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [01:16<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss 0.0133056640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/461 [00:01<03:13,  2.37it/s]\n"
     ]
    }
   ],
   "source": [
    "model_tuned = train(model.cuda().bfloat16(), dl_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation  \n",
    "\n",
    "Typically, we save the checkpoint and evaluate it later. However, direct evaluation is an option if minimal randomness is acceptable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tuned.save_ckpt(odir / \"model.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use organisim hg38; max sequence length including cls is 6392\n",
      "Warning: zhihan1996/DNABERT-2-117M does not exist! Try to use huggingface cached...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangdongxu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Some weights of the model checkpoint at zhihan1996/DNABERT-2-117M were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from tmp_eqtl/model.ckpt\n",
      "Loaded 290/290 parameters\n",
      "{'total_params': 191152515, 'trainable_params': 191152515}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 191152515, 'trainable_params': 191152515}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mc.init_model(finetune_ckpt = odir / \"model.ckpt\", dropout=0).cuda().bfloat16()\n",
    "model.display_trainable_parameters(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/307 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 307/307 [00:19<00:00, 15.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc 0.8792105598504799, aupr 0.8742700445408895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = model.cuda().bfloat16()\n",
    "probs = []\n",
    "labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(dl_test)):\n",
    "        keys_to_cuda = [\"input_ids\", \"position_ids\", \"label\"]\n",
    "        for k in keys_to_cuda:\n",
    "            batch[k] = batch[k].cuda()\n",
    "        logits = model(batch).view(-1)\n",
    "        probs.append(logits.sigmoid().float().cpu().numpy())\n",
    "        labels.append(batch[\"label\"].cpu().numpy())\n",
    "\n",
    "probs = np.concatenate(probs)\n",
    "labels = np.concatenate(labels)\n",
    "auc = metrics.roc_auc_score(labels, probs)\n",
    "aupr = metrics.average_precision_score(labels, probs)\n",
    "print(f\"test auc {auc}, aupr {aupr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification of differential regulators\n",
    "\n",
    "We can infer differential regulators for causal and non-causal variants by comparing distances in regulator embeddings. Notably, DNase-seq data often shows a significant difference in these distances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChromBERTEmbedding(\n",
       "  (pretrain_model): ChromBERT(\n",
       "    (embedding): BERTEmbedding(\n",
       "      (token): TokenEmbedding(10, 768, padding_idx=0)\n",
       "      (position): PositionalEmbedding(\n",
       "        (pe): PositionalEmbeddingTrainable(\n",
       "          (pe): Embedding(6392, 768)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0-7): 8 x EncoderTransformerBlock(\n",
       "        (attention): SelfAttentionFlashMHA(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (activation): GELU()\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (CistromeEmbeddingManager): CistromeEmbeddingManager()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_emb = model.get_embedding_manager()\n",
    "model_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update path: hdf5_file = hg38_6k_1kb.hdf5\n",
      "update path: meta_file = config/hg38_6k_meta.json\n",
      "update path: fasta_file = other/hg38.fa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 768/768 [00:34<00:00, 22.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1064, 768), (1064, 768))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc = chrombert.get_preset_dataset_config(\"prompt_dna\", supervised_file = table_eqtls)\n",
    "dl = dc.init_dataloader(batch_size = 2, shuffle=False, num_workers=4)\n",
    "labels = []\n",
    "embeddings = []\n",
    "for batch in tqdm(dl):\n",
    "    keys_to_cuda = [\"input_ids\", \"position_ids\", \"label\"]\n",
    "    for k in keys_to_cuda:\n",
    "        batch[k] = batch[k].cuda()\n",
    "    labels.append(batch[\"label\"].cpu().numpy())\n",
    "    embeddings.append(model_emb(batch).float().cpu().numpy())\n",
    "labels = np.concatenate(labels)\n",
    "embeddings = np.concatenate(embeddings)\n",
    "emb_causal = embeddings[labels == 1].mean(axis=0)\n",
    "emb_noncausal = embeddings[labels == 0].mean(axis=0)\n",
    "emb_causal.shape, emb_noncausal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regulator</th>\n",
       "      <th>shift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h3k4me3</td>\n",
       "      <td>0.213028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dnase</td>\n",
       "      <td>0.212970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>faire</td>\n",
       "      <td>0.207958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h3k4me2</td>\n",
       "      <td>0.199916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>maz</td>\n",
       "      <td>0.197446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kdm2b</td>\n",
       "      <td>0.197140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chd8</td>\n",
       "      <td>0.196293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>zfx</td>\n",
       "      <td>0.193517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gabpa</td>\n",
       "      <td>0.191083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nrf1</td>\n",
       "      <td>0.190021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>elf1</td>\n",
       "      <td>0.183495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tcf7l2</td>\n",
       "      <td>0.181039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rbl1</td>\n",
       "      <td>0.178543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max</td>\n",
       "      <td>0.178459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>zbtb7a</td>\n",
       "      <td>0.178251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>h2a</td>\n",
       "      <td>0.177546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>h3k18ac</td>\n",
       "      <td>0.176641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>h2az</td>\n",
       "      <td>0.176246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>myc</td>\n",
       "      <td>0.175153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>kmt2d</td>\n",
       "      <td>0.173348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   regulator     shift\n",
       "0    h3k4me3  0.213028\n",
       "1      dnase  0.212970\n",
       "2      faire  0.207958\n",
       "3    h3k4me2  0.199916\n",
       "4        maz  0.197446\n",
       "5      kdm2b  0.197140\n",
       "6       chd8  0.196293\n",
       "7        zfx  0.193517\n",
       "8      gabpa  0.191083\n",
       "9       nrf1  0.190021\n",
       "10      elf1  0.183495\n",
       "11    tcf7l2  0.181039\n",
       "12      rbl1  0.178543\n",
       "13       max  0.178459\n",
       "14    zbtb7a  0.178251\n",
       "15       h2a  0.177546\n",
       "16   h3k18ac  0.176641\n",
       "17      h2az  0.176246\n",
       "18       myc  0.175153\n",
       "19     kmt2d  0.173348"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shift = pd.DataFrame(\n",
    "    {\n",
    "        \"regulator\":model_emb.list_regulator,\n",
    "        \"shift\": 1- sklearn.metrics.pairwise.cosine_similarity(emb_causal, emb_noncausal).diagonal(),\n",
    "    }\n",
    ").sort_values(\"shift\", ascending=False, ignore_index=True)\n",
    "\n",
    "df_shift.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
